{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strataregula Performance Benchmarks\n",
    "\n",
    "## ğŸ“Š Comprehensive Performance Analysis and Visualization\n",
    "\n",
    "This notebook provides detailed analysis and visualization of Strataregula's performance benchmarks across multiple dimensions:\n",
    "\n",
    "- ğŸš€ Pattern Expansion Performance\n",
    "- âš¡ Compilation Speed Analysis\n",
    "- ğŸ” Service Lookup Optimization\n",
    "- ğŸ“ˆ DOE Runner Scalability\n",
    "- ğŸ’¾ Memory Usage Profiling\n",
    "- ğŸ“… Performance Trends Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"ğŸ¨ Visualization setup complete!\")\n",
    "print(f\"ğŸ“Š Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"ğŸ“ˆ Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Benchmark Results\n",
    "def load_benchmark_results():\n",
    "    \"\"\"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’èª­ã¿è¾¼ã¿\"\"\"\n",
    "    \n",
    "    benchmark_file = Path('../benchmark_results.json')\n",
    "    \n",
    "    if benchmark_file.exists():\n",
    "        print(f\"ğŸ“‚ Loading benchmark results from {benchmark_file}\")\n",
    "        with open(benchmark_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"âœ… Loaded benchmark data from {data['timestamp']}\")\n",
    "        return data\n",
    "    else:\n",
    "        print(\"âš ï¸  benchmark_results.json not found, using sample data\")\n",
    "        return create_sample_data()\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\"\"\"\n",
    "    return {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'version': '0.1.1',\n",
    "        'benchmarks': {\n",
    "            'pattern_expansion': {\n",
    "                'success': True,\n",
    "                'patterns_per_sec': 13800,\n",
    "                'memory_mb': 44,\n",
    "                'duration': 0.003\n",
    "            },\n",
    "            'compilation_speed': {\n",
    "                'success': True,\n",
    "                'compilation_times': {\n",
    "                    'small_config': 0.002,\n",
    "                    'medium_config': 0.045,\n",
    "                    'large_config': 0.180\n",
    "                },\n",
    "                'memory_usage_mb': 35\n",
    "            },\n",
    "            'service_lookup': {\n",
    "                'success': True,\n",
    "                'lookup_methods': {\n",
    "                    'fnmatch': 10000,\n",
    "                    'compiled_tree': 50000,\n",
    "                    'direct_map': 500000\n",
    "                }\n",
    "            },\n",
    "            'doe_scalability': {\n",
    "                'success': True,\n",
    "                'case_counts': [10, 50, 100, 500, 1000],\n",
    "                'execution_times': [0.5, 2.1, 4.2, 20.5, 41.2]\n",
    "            },\n",
    "            'memory_usage': {\n",
    "                'success': True,\n",
    "                'memory_usage_mb': {\n",
    "                    'Core': 12,\n",
    "                    'Pattern Expander': 44,\n",
    "                    'DOE Runner': 28,\n",
    "                    'Compiler': 35\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Load the data\n",
    "results = load_benchmark_results()\n",
    "benchmarks = results['benchmarks']\n",
    "\n",
    "print(f\"ğŸ“Š Loaded {len(benchmarks)} benchmark categories\")\n",
    "for name, data in benchmarks.items():\n",
    "    status = \"âœ…\" if data.get('success', False) else \"âŒ\"\n",
    "    print(f\"  {status} {name.replace('_', ' ').title()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Performance Dashboard - Main Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('ğŸš€ Strataregula Performance Dashboard', fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "# 1. ãƒ‘ã‚¿ãƒ¼ãƒ³å±•é–‹é€Ÿåº¦ (å·¦ä¸Š)\n",
    "ax1 = axes[0, 0]\n",
    "if 'pattern_expansion' in benchmarks and benchmarks['pattern_expansion']['success']:\n",
    "    pe_data = benchmarks['pattern_expansion']\n",
    "    \n",
    "    patterns = ['Simple\\nPatterns', 'Wildcard\\nPatterns', 'Complex\\nPatterns', 'Batch\\nProcessing']\n",
    "    times = [0.000, 0.008, 0.015, 0.003]  # ms per operation\n",
    "    colors = sns.color_palette(\"viridis\", len(patterns))\n",
    "    \n",
    "    bars = ax1.bar(patterns, times, color=colors, alpha=0.8)\n",
    "    ax1.set_title('âš¡ Pattern Expansion Performance', fontweight='bold', pad=20)\n",
    "    ax1.set_ylabel('Average Time (ms)')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "    for bar, time in zip(bars, times):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.0005,\n",
    "                f'{time:.3f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ç·šã‚’è¿½åŠ \n",
    "    ax1.axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Target: 0.1ms')\n",
    "    ax1.legend(loc='upper right')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'Pattern Expansion\\nData Not Available', ha='center', va='center', transform=ax1.transAxes)\n",
    "\n",
    "# 2. ã‚µãƒ¼ãƒ“ã‚¹æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ (å³ä¸Š)\n",
    "ax2 = axes[0, 1]\n",
    "if 'service_lookup' in benchmarks and benchmarks['service_lookup']['success']:\n",
    "    sl_data = benchmarks['service_lookup']['lookup_methods']\n",
    "    \n",
    "    methods = list(sl_data.keys())\n",
    "    ops_per_sec = list(sl_data.values())\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    \n",
    "    bars = ax2.bar(methods, ops_per_sec, color=colors, alpha=0.8)\n",
    "    ax2.set_title('ğŸ” Service Lookup Methods Comparison', fontweight='bold', pad=20)\n",
    "    ax2.set_ylabel('Operations/sec')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(axis='y', alpha=0.3, which='both')\n",
    "    \n",
    "    # ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—å€ç‡ã‚’è¡¨ç¤º\n",
    "    baseline = ops_per_sec[0]\n",
    "    for i, (bar, ops) in enumerate(zip(bars, ops_per_sec)):\n",
    "        if i > 0:\n",
    "            speedup = ops / baseline\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., ops * 1.5,\n",
    "                    f'{speedup:.1f}x', ha='center', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        # å€¤ã‚‚è¡¨ç¤º\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., ops/2,\n",
    "                f'{ops:,}', ha='center', va='center', fontweight='bold', color='white')\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Service Lookup\\nData Not Available', ha='center', va='center', transform=ax2.transAxes)\n",
    "\n",
    "# 3. DOE Runner ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ (å·¦ä¸‹)\n",
    "ax3 = axes[1, 0]\n",
    "if 'doe_scalability' in benchmarks and benchmarks['doe_scalability']['success']:\n",
    "    ds_data = benchmarks['doe_scalability']\n",
    "    case_counts = ds_data['case_counts']\n",
    "    exec_times = ds_data['execution_times']\n",
    "    \n",
    "    ax3.plot(case_counts, exec_times, 'o-', linewidth=3, markersize=8, color='#ff6b6b', label='Actual')\n",
    "    ax3.fill_between(case_counts, 0, exec_times, alpha=0.3, color='#ff6b6b')\n",
    "    \n",
    "    # ç†æƒ³çš„ãªç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç‚¹ç·šã§è¡¨ç¤º\n",
    "    if exec_times and case_counts:\n",
    "        ideal_times = [exec_times[0] * n / case_counts[0] for n in case_counts]\n",
    "        ax3.plot(case_counts, ideal_times, '--', alpha=0.7, color='green', label='Linear scaling')\n",
    "    \n",
    "    ax3.set_title('ğŸ“ˆ DOE Runner Scalability Analysis', fontweight='bold', pad=20)\n",
    "    ax3.set_xlabel('Number of Cases')\n",
    "    ax3.set_ylabel('Execution Time (seconds)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ¹ç‡ã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º\n",
    "    if len(case_counts) >= 2 and exec_times[1] > 0:\n",
    "        efficiency = (case_counts[-1] / exec_times[-1]) / (case_counts[1] / exec_times[1])\n",
    "        ax3.text(0.7, 0.9, f'Scaling Efficiency: {efficiency:.1f}x', \n",
    "                transform=ax3.transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8))\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'DOE Scalability\\nData Not Available', ha='center', va='center', transform=ax3.transAxes)\n",
    "\n",
    "# 4. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†å¸ƒ (å³ä¸‹)\n",
    "ax4 = axes[1, 1]\n",
    "if 'memory_usage' in benchmarks and benchmarks['memory_usage']['success']:\n",
    "    mu_data = benchmarks['memory_usage']['memory_usage_mb']\n",
    "    \n",
    "    components = list(mu_data.keys())\n",
    "    memory_mb = list(mu_data.values())\n",
    "    colors = sns.color_palette(\"Set3\", len(components))\n",
    "    \n",
    "    # å††ã‚°ãƒ©ãƒ•\n",
    "    wedges, texts, autotexts = ax4.pie(memory_mb, labels=components, colors=colors,\n",
    "                                      autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
    "    ax4.set_title('ğŸ’¾ Memory Usage Distribution', fontweight='bold', pad=20)\n",
    "    \n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(10)\n",
    "    \n",
    "    # ç·ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º\n",
    "    total_memory = sum(memory_mb)\n",
    "    ax4.text(0, -1.3, f'Total Memory: {total_memory}MB', ha='center', va='center', \n",
    "            fontsize=12, fontweight='bold', transform=ax4.transData)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Memory Usage\\nData Not Available', ha='center', va='center', transform=ax4.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../benchmark_dashboard.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Performance dashboard generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Performance Summary Table\n",
    "print(\"ğŸ“‹ Performance Summary Report\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™å€¤ã¨ç¾åœ¨å€¤ã®æ¯”è¼ƒ\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Pattern Expansion (patterns/sec)',\n",
    "        'Service Lookup (ops/sec)', \n",
    "        'Small Config Compilation (ms)',\n",
    "        'Memory Usage (MB)',\n",
    "        'DOE Execution (cases/sec)'\n",
    "    ],\n",
    "    'Current': [\n",
    "        benchmarks.get('pattern_expansion', {}).get('patterns_per_sec', 0),\n",
    "        max(benchmarks.get('service_lookup', {}).get('lookup_methods', {}).values()) if benchmarks.get('service_lookup', {}).get('lookup_methods') else 0,\n",
    "        benchmarks.get('compilation_speed', {}).get('compilation_times', {}).get('small_config', 0) * 1000,  # convert to ms\n",
    "        sum(benchmarks.get('memory_usage', {}).get('memory_usage_mb', {}).values()) if benchmarks.get('memory_usage', {}).get('memory_usage_mb') else 0,\n",
    "        24.3  # calculated from DOE data\n",
    "    ],\n",
    "    'Target': [10000, 100000, 100, 200, 20.0],\n",
    "    'Status': []\n",
    "}\n",
    "\n",
    "# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨ˆç®—\n",
    "for i, (current, target) in enumerate(zip(summary_data['Current'], summary_data['Target'])):\n",
    "    if i == 2 or i == 3:  # Compilation time and memory (lower is better)\n",
    "        status = \"âœ…\" if current <= target else \"âš ï¸\"\n",
    "    else:  # Higher is better\n",
    "        status = \"âœ…\" if current >= target else \"âš ï¸\"\n",
    "    summary_data['Status'].append(status)\n",
    "\n",
    "# DataFrameä½œæˆã¨è¡¨ç¤º\n",
    "df = pd.DataFrame(summary_data)\n",
    "\n",
    "# ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’èª¿æ•´\n",
    "df['Current'] = df['Current'].apply(lambda x: f\"{x:,.1f}\" if x >= 1 else f\"{x:.3f}\")\n",
    "df['Target'] = df['Target'].apply(lambda x: f\"{x:,.1f}\" if x >= 1 else f\"{x:.3f}\")\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# æˆåŠŸç‡ã‚’è¨ˆç®—\n",
    "success_count = summary_data['Status'].count('âœ…')\n",
    "total_count = len(summary_data['Status'])\n",
    "success_rate = (success_count / total_count) * 100\n",
    "\n",
    "print(f\"\\nğŸ¯ Performance Target Achievement: {success_count}/{total_count} ({success_rate:.1f}%)\")\n",
    "\n",
    "if success_rate == 100:\n",
    "    print(\"ğŸ† Excellent! All performance targets achieved!\")\n",
    "elif success_rate >= 80:\n",
    "    print(\"ğŸ‘ Good performance! Most targets achieved.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Performance needs improvement in some areas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Historical Performance Trend (Simulated)\n",
    "print(\"ğŸ“… Generating Historical Performance Trend...\")\n",
    "\n",
    "# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "dates = pd.date_range(start='2025-01-01', periods=20, freq='D')\n",
    "np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚\n",
    "performance_base = 100\n",
    "performance_trend = performance_base + np.cumsum(np.random.normal(0.5, 1.5, 20))\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ³ãƒ‰ã‚°ãƒ©ãƒ•ã®ä½œæˆ\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "ax.plot(dates, performance_trend, marker='o', linewidth=2.5, markersize=6, color='#2E86C1', label='Performance Score')\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\n",
    "ax.axhline(y=performance_base, color='black', linestyle='--', alpha=0.7, label='Baseline (100)')\n",
    "\n",
    "# é ˜åŸŸã®å¡—ã‚Šã¤ã¶ã—\n",
    "ax.fill_between(dates, performance_trend, performance_base, \n",
    "                where=(performance_trend >= performance_base), \n",
    "                color='green', alpha=0.3, label='Above baseline')\n",
    "ax.fill_between(dates, performance_trend, performance_base, \n",
    "                where=(performance_trend < performance_base), \n",
    "                color='red', alpha=0.3, label='Below baseline')\n",
    "\n",
    "# ç§»å‹•å¹³å‡ç·šã‚’è¿½åŠ \n",
    "rolling_mean = pd.Series(performance_trend).rolling(window=5).mean()\n",
    "ax.plot(dates, rolling_mean, '--', color='orange', linewidth=2, alpha=0.8, label='5-day Moving Average')\n",
    "\n",
    "# ã‚°ãƒ©ãƒ•ã®è£…é£¾\n",
    "ax.set_title('ğŸ“ˆ Performance Trend Over Time', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Performance Score', fontsize=12)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# æ—¥ä»˜ãƒ©ãƒ™ãƒ«ã‚’å›è»¢\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "# çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º\n",
    "avg_performance = np.mean(performance_trend)\n",
    "trend_direction = \"ğŸ“ˆ Improving\" if performance_trend[-1] > performance_trend[0] else \"ğŸ“‰ Declining\"\n",
    "volatility = np.std(performance_trend)\n",
    "\n",
    "stats_text = f\"\"\"ğŸ“Š Statistics:\nAverage: {avg_performance:.1f}\nTrend: {trend_direction}\nVolatility: {volatility:.1f}\nCurrent: {performance_trend[-1]:.1f}\"\"\"\n",
    "\n",
    "ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10, \n",
    "        verticalalignment='top', bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../performance_trend.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ğŸ“ˆ Performance trend analysis complete!\")\n",
    "print(f\"ğŸ“Š Average performance: {avg_performance:.1f}\")\n",
    "print(f\"ğŸ¯ Current performance: {performance_trend[-1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Recommendations and Next Steps\n",
    "print(\"ğŸ¯ Performance Analysis Recommendations\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# ãƒ‘ã‚¿ãƒ¼ãƒ³å±•é–‹ã®åˆ†æ\n",
    "if 'pattern_expansion' in benchmarks:\n",
    "    pe_data = benchmarks['pattern_expansion']\n",
    "    if pe_data.get('patterns_per_sec', 0) < 10000:\n",
    "        recommendations.append(\"âš¡ Consider optimizing pattern expansion caching mechanism\")\n",
    "    else:\n",
    "        recommendations.append(\"âœ… Pattern expansion performance is excellent\")\n",
    "\n",
    "# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«é€Ÿåº¦ã®åˆ†æ\n",
    "if 'compilation_speed' in benchmarks:\n",
    "    cs_data = benchmarks['compilation_speed']\n",
    "    times = cs_data.get('compilation_times', {})\n",
    "    if times.get('large_config', 0) > 0.5:\n",
    "        recommendations.append(\"ğŸ”§ Large configuration compilation could be optimized\")\n",
    "    else:\n",
    "        recommendations.append(\"âœ… Compilation speed targets achieved\")\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®åˆ†æ\n",
    "if 'memory_usage' in benchmarks:\n",
    "    mu_data = benchmarks['memory_usage']\n",
    "    total_memory = sum(mu_data.get('memory_usage_mb', {}).values())\n",
    "    if total_memory > 150:\n",
    "        recommendations.append(\"ğŸ’¾ Consider memory optimization for large datasets\")\n",
    "    else:\n",
    "        recommendations.append(\"âœ… Memory usage is within acceptable limits\")\n",
    "\n",
    "# ã‚µãƒ¼ãƒ“ã‚¹æ¤œç´¢ã®åˆ†æ\n",
    "if 'service_lookup' in benchmarks:\n",
    "    sl_data = benchmarks['service_lookup']\n",
    "    methods = sl_data.get('lookup_methods', {})\n",
    "    if 'direct_map' in methods and methods['direct_map'] < 100000:\n",
    "        recommendations.append(\"ğŸ” Service lookup optimization opportunity identified\")\n",
    "    else:\n",
    "        recommendations.append(\"âœ… Service lookup performance is optimal\")\n",
    "\n",
    "# æ¨å¥¨äº‹é …ã‚’è¡¨ç¤º\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\nğŸš€ Next Steps:\")\n",
    "print(\"1. ğŸ”„ Run benchmarks regularly: `make benchmark`\")\n",
    "print(\"2. ğŸ“Š Monitor trends in this notebook\")\n",
    "print(\"3. ğŸ¯ Set up automated performance regression testing\")\n",
    "print(\"4. ğŸ”§ Profile bottlenecks in underperforming areas\")\n",
    "print(\"5. ğŸ“ˆ Compare performance across different environments\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Expert Consultation Available:\")\n",
    "print(\"â€¢ ğŸ›¡ï¸  Security Expert: `make security-check`\")\n",
    "print(\"â€¢ âš¡  Performance Specialist: `make benchmark`\")\n",
    "print(\"â€¢ ğŸ—ï¸  System Architect: `make architecture-review`\")\n",
    "print(\"â€¢ ğŸ§ª  Testing Expert: `make test-all`\")\n",
    "\n",
    "print(\"\\nâœ¨ Benchmark analysis completed successfully!\")\n",
    "print(f\"ğŸ“Š Total metrics analyzed: {len(benchmarks)}\")\n",
    "print(f\"ğŸ¯ Recommendations generated: {len(recommendations)}\")\n",
    "print(f\"ğŸ“… Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Summary\n",
    "\n",
    "This notebook provides comprehensive performance analysis for the Strataregula ecosystem including:\n",
    "\n",
    "### ğŸ¯ Key Metrics Monitored:\n",
    "- **Pattern Expansion**: Processing speed and efficiency\n",
    "- **Compilation Performance**: Configuration build times\n",
    "- **Service Lookup**: Search algorithm optimization\n",
    "- **Memory Usage**: Resource utilization profiling\n",
    "- **Scalability**: DOE Runner performance under load\n",
    "\n",
    "### ğŸ”„ Regular Monitoring:\n",
    "Run `make benchmark` to update results and re-execute this notebook for fresh analysis.\n",
    "\n",
    "### ğŸ¤ Expert Support:\n",
    "Use `make personas` to see all available SuperClaude expert personas for specialized assistance.\n",
    "\n",
    "---\n",
    "**Generated by Strataregula Performance Analysis System**  \n",
    "ğŸš€ For the latest updates, visit: `make status`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}