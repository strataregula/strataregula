#!/usr/bin/env python3
"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¨ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
"""

import sys
import subprocess
from pathlib import Path
import json
import time

def test_component(name, test_func):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print(f"Testing {name}...", end=" ")
    start_time = time.time()
    
    try:
        success = test_func()
        duration = time.time() - start_time
        
        if success:
            print(f"PASS ({duration:.3f}s)")
            return True
        else:
            print(f"FAIL ({duration:.3f}s)")
            return False
    except Exception as e:
        duration = time.time() - start_time
        print(f"ERROR ({duration:.3f}s): {e}")
        return False

def test_benchmark_execution():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚’ãƒ†ã‚¹ãƒˆ"""
    try:
        result = subprocess.run([
            sys.executable, "scripts/run_benchmarks.py"
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            # benchmark_results.json ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            return Path('benchmark_results.json').exists()
        return False
    except subprocess.TimeoutExpired:
        return False

def test_image_generation():
    """ç”»åƒç”Ÿæˆã‚’ãƒ†ã‚¹ãƒˆ"""
    try:
        result = subprocess.run([
            sys.executable, "scripts/generate_benchmark_images.py"
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            images_dir = Path('docs/images')
            expected_files = [
                'benchmark_performance.png',
                'benchmark_scalability.png', 
                'benchmark_compilation.png',
                'benchmark_memory.png',
                'benchmark_dashboard.png'
            ]
            
            return all((images_dir / img).exists() for img in expected_files)
        return False
    except subprocess.TimeoutExpired:
        return False

def test_notebook_conversion():
    """Notebookå¤‰æ›ã‚’ãƒ†ã‚¹ãƒˆ"""
    try:
        result = subprocess.run([
            sys.executable, "scripts/convert_notebook.py"
        ], capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0:
            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            return Path('docs/benchmark.html').exists()
        return False
    except subprocess.TimeoutExpired:
        return False

def test_benchmark_data_quality():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ãƒ†ã‚¹ãƒˆ"""
    benchmark_file = Path('benchmark_results.json')
    
    if not benchmark_file.exists():
        return False
    
    try:
        with open(benchmark_file, 'r') as f:
            data = json.load(f)
        
        # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
        required_fields = ['timestamp', 'version', 'benchmarks']
        if not all(field in data for field in required_fields):
            return False
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ç¢ºèª
        benchmarks = data['benchmarks']
        expected_benchmarks = [
            'pattern_expansion',
            'compilation_speed', 
            'service_lookup',
            'doe_scalability',
            'memory_usage'
        ]
        
        if not all(bench in benchmarks for bench in expected_benchmarks):
            return False
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã®ç¢ºèª
        pe = benchmarks.get('pattern_expansion', {})
        if pe.get('patterns_per_sec', 0) < 10000:
            return False
        
        sl = benchmarks.get('service_lookup', {})
        if not sl.get('lookup_methods', {}).get('direct_map', 0) >= 100000:
            return False
        
        return True
    except (json.JSONDecodeError, KeyError):
        return False

def test_makefile_commands():
    """Makefileã‚³ãƒãƒ³ãƒ‰ã®å­˜åœ¨ã‚’ãƒ†ã‚¹ãƒˆ"""
    makefile_path = Path('Makefile')
    if not makefile_path.exists():
        return False
    
    with open(makefile_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å¿…è¦ãªã‚³ãƒãƒ³ãƒ‰ã®ç¢ºèª
    required_commands = [
        'benchmark',
        'benchmark-images',
        'benchmark-view',
        'security-check',
        'personas',
        'status'
    ]
    
    return all(f"{cmd}:" in content for cmd in required_commands)

def test_readme_integration():
    """README.mdã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æƒ…å ±ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒ†ã‚¹ãƒˆ"""
    readme_path = Path('README.md')
    if not readme_path.exists():
        return False
    
    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å¿…è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    required_sections = [
        '## ğŸ“Š Performance Benchmarks',
        'benchmark_performance.png',
        'benchmark_scalability.png',
        'All performance targets achieved'
    ]
    
    return all(section in content for section in required_sections)

def test_github_actions_workflow():
    """GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å­˜åœ¨ã‚’ãƒ†ã‚¹ãƒˆ"""
    workflow_path = Path('.github/workflows/benchmark.yml')
    if not workflow_path.exists():
        return False
    
    with open(workflow_path, 'r') as f:
        content = f.read()
    
    # å¿…è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã®ç¢ºèª
    required_steps = [
        'Run benchmarks',
        'Generate visualizations',
        'Convert notebook to HTML',
        'Commit updated visualizations'
    ]
    
    return all(step in content for step in required_steps)

def run_comprehensive_test():
    """åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("Strataregula Benchmark Visualization System Test")
    print("=" * 55)
    
    test_cases = [
        ("Benchmark Execution", test_benchmark_execution),
        ("Image Generation", test_image_generation), 
        ("Notebook Conversion", test_notebook_conversion),
        ("Benchmark Data Quality", test_benchmark_data_quality),
        ("Makefile Commands", test_makefile_commands),
        ("README Integration", test_readme_integration),
        ("GitHub Actions Workflow", test_github_actions_workflow)
    ]
    
    results = []
    total_start_time = time.time()
    
    for name, test_func in test_cases:
        success = test_component(name, test_func)
        results.append((name, success))
    
    total_duration = time.time() - total_start_time
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 55)
    print("Test Results Summary:")
    print("-" * 30)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for name, success in results:
        status = "PASS" if success else "FAIL"
        print(f"  {name:<25} {status}")
    
    print("-" * 30)
    print(f"Total: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    print(f"Duration: {total_duration:.3f}s")
    
    if passed == total:
        print("\nAll systems operational!")
        print("Benchmark visualization system ready for production")
        print("\nNext steps:")
        print("1. git add .")
        print("2. git commit -m 'Complete benchmark visualization system'")  
        print("3. git push")
        print("4. Enable GitHub Pages for automatic publishing")
        return 0
    else:
        print(f"\n!! {total - passed} test(s) failed!")
        print("Please check the failing components before deployment")
        return 1

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        return run_comprehensive_test()
    except KeyboardInterrupt:
        print("\n\nTest interrupted by user")
        return 1
    except Exception as e:
        print(f"\nUnexpected error during testing: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())