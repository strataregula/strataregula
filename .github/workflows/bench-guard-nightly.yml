name: Bench Guard Nightly
# StrataRegula é•·æ™‚é–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
# æ¯æ—¥æ·±å¤œã«å®Ÿè¡Œã—ã€è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨ãƒˆãƒ¬ãƒ³ãƒ‰ç›£è¦–ã‚’è¡Œã†

on:
  schedule:
    # æ¯æ—¥æ·±å¤œ2æ™‚ï¼ˆJSTï¼‰ã«å®Ÿè¡Œï¼ˆUTC 17:00ï¼‰
    - cron: '0 17 * * *'
  workflow_dispatch:
    # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½
    inputs:
      test_scale:
        description: 'Test scale (light/standard/heavy)'
        required: false
        default: 'standard'
        type: choice
        options:
        - light
        - standard  
        - heavy

jobs:
  nightly-performance:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      TZ: Asia/Tokyo
      PYTHONUTF8: "1"
      # Nightlyç”¨ã®å³æ ¼ãªè¨­å®šï¼ˆã‚ˆã‚Šå¤šãã®å®Ÿè¡Œã€å³ã—ã„é–¾å€¤ï¼‰
      SR_BENCH_MIN_RATIO: "60"      # é€šå¸¸ã‚ˆã‚Šé«˜ã„è¦æ±‚ï¼ˆ60xï¼‰
      SR_BENCH_MAX_P95_US: "40"     # é€šå¸¸ã‚ˆã‚Šå³ã—ã„ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦æ±‚ï¼ˆ40Î¼sï¼‰
      SR_BENCH_MAX_P99_US: "100"    # p99ã‚‚ç›£è¦–ï¼ˆ100Î¼sï¼‰
      SR_BENCH_N: "100000"          # 10ä¸‡å›å®Ÿè¡Œã§è©³ç´°åˆ†æ
      SR_BENCH_WARMUP: "5000"       # å……åˆ†ãªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set test parameters
        id: params
        run: |
          # æ‰‹å‹•å®Ÿè¡Œæ™‚ã®è¨­å®šèª¿æ•´
          case "${{ github.event.inputs.test_scale }}" in
            light)
              echo "SR_BENCH_N=10000" >> $GITHUB_ENV
              echo "SR_BENCH_WARMUP=1000" >> $GITHUB_ENV
              echo "TIMEOUT_MINUTES=30" >> $GITHUB_ENV
              echo "ğŸŸ¡ Light test scale selected"
              ;;
            heavy)
              echo "SR_BENCH_N=500000" >> $GITHUB_ENV
              echo "SR_BENCH_WARMUP=10000" >> $GITHUB_ENV
              echo "TIMEOUT_MINUTES=120" >> $GITHUB_ENV
              echo "ğŸ”´ Heavy test scale selected"
              ;;
            *)
              echo "ğŸŸ¢ Standard test scale (default)"
              ;;
          esac
          
          # å®Ÿè¡Œæ™‚åˆ»ã‚’è¨˜éŒ²
          echo "NIGHTLY_START=$(date '+%Y-%m-%d %H:%M:%S JST')" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          
          # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨è¿½åŠ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
          pip install psutil matplotlib seaborn pandas

      - name: System performance baseline
        run: |
          echo "ğŸ“Š System Performance Baseline"
          echo "=============================="
          echo "ğŸ–¥ï¸  CPU Info:"
          lscpu | head -20
          echo ""
          echo "ğŸ’¾ Memory Info:"
          free -h
          echo ""
          echo "ğŸ’½ Disk Info:"
          df -h
          echo ""
          echo "ğŸ Python Info:"
          python --version
          python -c "import sys; print(f'Python executable: {sys.executable}')"
          python -c "import platform; print(f'Platform: {platform.platform()}')"

      - name: Run comprehensive performance test
        id: nightly-bench
        run: |
          echo "ğŸŒ™ Starting Nightly Performance Analysis"
          echo "========================================"
          echo "â° Start time: $NIGHTLY_START"
          echo "ğŸ¯ Target ratio: â‰¥${SR_BENCH_MIN_RATIO}x"
          echo "ğŸ¯ Target p95: â‰¤${SR_BENCH_MAX_P95_US}Î¼s"
          echo "ğŸ¯ Target p99: â‰¤${SR_BENCH_MAX_P99_US}Î¼s"
          echo "ğŸ”¢ Test iterations: ${SR_BENCH_N}"
          echo "ğŸ”¥ Warmup iterations: ${SR_BENCH_WARMUP}"
          echo ""
          
          # 5å›å®Ÿè¡Œã—ã¦è©³ç´°åˆ†æï¼ˆNightlyã¯æ™‚é–“ã«ä½™è£•ãŒã‚ã‚‹ãŸã‚ï¼‰
          results=()
          for run in 1 2 3 4 5; do
            echo "ğŸ”„ Running comprehensive test $run/5..."
            
            if python scripts/bench_guard.py; then
              mv bench_guard.json "nightly_run${run}.json"
              echo "âœ… Run $run completed successfully"
            else
              echo "âš ï¸ Run $run encountered issues, saving partial results..."
              if [ -f bench_guard.json ]; then
                mv bench_guard.json "nightly_run${run}.json"
              else
                echo "{\"overall\":{\"min_ratio\":0,\"max_p95_us\":9999},\"passed\":false,\"error\":\"execution_failed\",\"run\":$run}" > "nightly_run${run}.json"
              fi
            fi
            
            # çŸ­ã„é–“éš”ã§GCã‚’å®Ÿè¡Œ
            python -c "import gc; gc.collect()"
            sleep 5
          done
          
          echo ""
          echo "ğŸ§® Analyzing results from 5 comprehensive runs..."

      - name: Advanced statistical analysis
        run: |
          python - <<'PY'
          import json
          import glob
          import statistics
          import sys
          import os
          from datetime import datetime
          import math
          
          def analyze_performance():
              # 5ã¤ã®çµæœã‚’åé›†
              run_files = sorted(glob.glob("nightly_run*.json"))
              print(f"ğŸ“Š Found {len(run_files)} result files")
              
              runs = []
              valid_runs = 0
              
              for file in run_files:
                  try:
                      with open(file) as f:
                          data = json.load(f)
                          runs.append(data)
                          if data.get('passed', False):
                              valid_runs += 1
                      print(f"âœ… {file}: loaded successfully")
                  except Exception as e:
                      print(f"âŒ {file}: failed to load - {e}")
                      continue
              
              if len(runs) < 3:
                  print(f"âŒ Insufficient data: only {len(runs)} runs available")
                  sys.exit(1)
              
              # çµ±è¨ˆåˆ†æ
              ratios = [r.get("overall", {}).get("min_ratio", 0) for r in runs]
              p95s = [r.get("overall", {}).get("max_p95_us", 9999) for r in runs]
              
              # è©³ç´°çµ±è¨ˆ
              stats = {
                  "ratio": {
                      "mean": statistics.mean(ratios),
                      "median": statistics.median(ratios),
                      "stdev": statistics.stdev(ratios) if len(ratios) > 1 else 0,
                      "min": min(ratios),
                      "max": max(ratios),
                      "values": ratios
                  },
                  "p95": {
                      "mean": statistics.mean(p95s),
                      "median": statistics.median(p95s), 
                      "stdev": statistics.stdev(p95s) if len(p95s) > 1 else 0,
                      "min": min(p95s),
                      "max": max(p95s),
                      "values": p95s
                  }
              }
              
              # é–¾å€¤
              ratio_threshold = float(os.getenv("SR_BENCH_MIN_RATIO", "60"))
              p95_threshold = float(os.getenv("SR_BENCH_MAX_P95_US", "40"))
              p99_threshold = float(os.getenv("SR_BENCH_MAX_P99_US", "100"))
              
              # åˆ¤å®šï¼ˆNightlyã¯ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
              median_ratio = stats["ratio"]["median"]
              median_p95 = stats["p95"]["median"]
              
              ratio_ok = median_ratio >= ratio_threshold
              p95_ok = median_p95 <= p95_threshold
              final_passed = ratio_ok and p95_ok
              
              # å®‰å®šæ€§è©•ä¾¡ï¼ˆå¤‰å‹•ä¿‚æ•°ï¼‰
              ratio_cv = (stats["ratio"]["stdev"] / stats["ratio"]["mean"]) * 100 if stats["ratio"]["mean"] > 0 else 100
              p95_cv = (stats["p95"]["stdev"] / stats["p95"]["mean"]) * 100 if stats["p95"]["mean"] > 0 else 100
              
              print(f"\nğŸ“Š Nightly Performance Analysis Results")
              print(f"==========================================")
              print(f"ğŸ” Test runs: {len(runs)} ({valid_runs} passed)")
              print(f"ğŸ“ˆ Speed ratio:")
              print(f"   - Median: {median_ratio:.2f}x (threshold: â‰¥{ratio_threshold}x) {'âœ…' if ratio_ok else 'âŒ'}")
              print(f"   - Mean: {stats['ratio']['mean']:.2f}x Â± {stats['ratio']['stdev']:.2f}")
              print(f"   - Range: {stats['ratio']['min']:.2f}x ~ {stats['ratio']['max']:.2f}x")
              print(f"   - Stability: CV={ratio_cv:.1f}% ({'Good' if ratio_cv < 10 else 'Variable' if ratio_cv < 20 else 'Unstable'})")
              print(f"ğŸ“‰ P95 latency:")
              print(f"   - Median: {median_p95:.1f}Î¼s (threshold: â‰¤{p95_threshold}Î¼s) {'âœ…' if p95_ok else 'âŒ'}")
              print(f"   - Mean: {stats['p95']['mean']:.1f}Î¼s Â± {stats['p95']['stdev']:.1f}")
              print(f"   - Range: {stats['p95']['min']:.1f}Î¼s ~ {stats['p95']['max']:.1f}Î¼s")
              print(f"   - Stability: CV={p95_cv:.1f}% ({'Good' if p95_cv < 15 else 'Variable' if p95_cv < 30 else 'Unstable'})")
              print(f"\nğŸ¯ Overall: {'âœ… PASSED' if final_passed else 'âŒ FAILED'}")
              
              # ç·åˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
              nightly_result = {
                  "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S JST"),
                  "test_type": "nightly_comprehensive",
                  "runs_analyzed": len(runs),
                  "valid_runs": valid_runs,
                  "statistics": stats,
                  "thresholds": {
                      "min_ratio": ratio_threshold,
                      "max_p95_us": p95_threshold,
                      "max_p99_us": p99_threshold
                  },
                  "evaluation": {
                      "median_ratio": median_ratio,
                      "median_p95": median_p95,
                      "ratio_ok": ratio_ok,
                      "p95_ok": p95_ok,
                      "stability": {
                          "ratio_cv": ratio_cv,
                          "p95_cv": p95_cv,
                          "overall_stable": ratio_cv < 15 and p95_cv < 20
                      }
                  },
                  "passed": final_passed,
                  "individual_runs": runs
              }
              
              with open("bench_guard_nightly.json", "w") as f:
                  json.dump(nightly_result, f, indent=2, ensure_ascii=False)
              
              print(f"\nğŸ’¾ Comprehensive results saved to bench_guard_nightly.json")
              
              # GitHub Actionså‡ºåŠ›
              print(f"NIGHTLY_RATIO={median_ratio:.2f}")
              print(f"NIGHTLY_P95={median_p95:.1f}")
              print(f"NIGHTLY_PASSED={str(final_passed).lower()}")
              print(f"STABILITY_RATIO_CV={ratio_cv:.1f}")
              print(f"STABILITY_P95_CV={p95_cv:.1f}")
              
              return final_passed
          
          success = analyze_performance()
          sys.exit(0 if success else 1)
          PY

      - name: Upload comprehensive results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: strataregula-nightly-bench-results-${{ env.NIGHTLY_START }}
          path: |
            bench_guard_nightly.json
            nightly_run*.json
          retention-days: 90

      - name: Performance trend analysis
        if: always()
        run: |
          echo "ğŸ“ˆ Performance Trend Analysis" >> $GITHUB_STEP_SUMMARY
          echo "============================" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "_Nightly comprehensive test completed at $(date '+%Y-%m-%d %H:%M:%S JST')_" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f bench_guard_nightly.json ]; then
            python - <<'PY' >> $GITHUB_STEP_SUMMARY
import json
from datetime import datetime

try:
    with open("bench_guard_nightly.json") as f:
        data = json.load(f)
    
    eval_data = data["evaluation"]
    stats = data["statistics"]
    stability = eval_data["stability"]
    
    # çµæœã‚µãƒãƒªãƒ¼
    passed = data["passed"]
    status_emoji = "âœ…" if passed else "âŒ"
    status_text = "PASSED" if passed else "FAILED"
    
    print(f"## {status_emoji} Nightly Result: **{status_text}**")
    print()
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print("### ğŸ“Š Performance Statistics (5-run analysis)")
    print("| Metric | Median | Mean Â± StdDev | Range | Stability |")
    print("|--------|--------|---------------|-------|-----------|")
    
    ratio_stats = stats["ratio"]
    p95_stats = stats["p95"]
    
    ratio_stability = "ğŸŸ¢ Good" if stability["ratio_cv"] < 10 else "ğŸŸ¡ Variable" if stability["ratio_cv"] < 20 else "ğŸ”´ Unstable"
    p95_stability = "ğŸŸ¢ Good" if stability["p95_cv"] < 15 else "ğŸŸ¡ Variable" if stability["p95_cv"] < 30 else "ğŸ”´ Unstable"
    
    print(f"| **Speed Ratio** | {ratio_stats['median']:.2f}x | {ratio_stats['mean']:.2f} Â± {ratio_stats['stdev']:.2f} | {ratio_stats['min']:.1f}x ~ {ratio_stats['max']:.1f}x | {ratio_stability} (CV: {stability['ratio_cv']:.1f}%) |")
    print(f"| **P95 Latency** | {p95_stats['median']:.1f}Î¼s | {p95_stats['mean']:.1f} Â± {p95_stats['stdev']:.1f} | {p95_stats['min']:.1f}Î¼s ~ {p95_stats['max']:.1f}Î¼s | {p95_stability} (CV: {stability['p95_cv']:.1f}%) |")
    
    print()
    
    # è©³ç´°è©•ä¾¡
    print("### ğŸ¯ Threshold Evaluation")
    thresholds = data["thresholds"]
    ratio_ok = eval_data["ratio_ok"]
    p95_ok = eval_data["p95_ok"]
    
    ratio_emoji = "âœ…" if ratio_ok else "âŒ"
    p95_emoji = "âœ…" if p95_ok else "âŒ"
    
    print(f"- {ratio_emoji} **Speed Performance**: {eval_data['median_ratio']:.2f}x (required: â‰¥{thresholds['min_ratio']}x)")
    print(f"- {p95_emoji} **Latency Performance**: {eval_data['median_p95']:.1f}Î¼s (required: â‰¤{thresholds['max_p95_us']}Î¼s)")
    
    overall_stable = stability["overall_stable"]
    stability_emoji = "âœ…" if overall_stable else "âš ï¸"
    print(f"- {stability_emoji} **Performance Stability**: {'Stable' if overall_stable else 'Variable'}")
    
    print()
    
    # è¨­å®šæƒ…å ±
    print("### âš™ï¸ Test Configuration")
    print(f"- **Test runs**: {data['runs_analyzed']} comprehensive tests")
    print(f"- **Valid runs**: {data['valid_runs']}")
    print(f"- **Test iterations**: {thresholds.get('test_iterations', 'N/A'):,} per run")
    print(f"- **Analysis method**: 5-run statistical analysis with median evaluation")
    print()
    
    if not passed:
        print("### âŒ Performance Issues Detected")
        print("**Nightly monitoring has detected performance degradation.**")
        print()
        if not ratio_ok:
            print(f"- **Speed Regression**: Median ratio {eval_data['median_ratio']:.2f}x below threshold")
        if not p95_ok:
            print(f"- **Latency Regression**: Median p95 {eval_data['median_p95']:.1f}Î¼s exceeds threshold")
        if not overall_stable:
            print(f"- **Performance Instability**: High variability detected (Ratio CV: {stability['ratio_cv']:.1f}%, P95 CV: {stability['p95_cv']:.1f}%)")
        print()
        print("**ğŸ” Investigation Required:**")
        print("1. Review recent changes for performance impact")
        print("2. Check for system resource constraints")
        print("3. Consider algorithm optimization opportunities")
    else:
        print("### âœ… Excellent Performance")
        print("All nightly performance targets achieved with good stability! ğŸ‰")
        if overall_stable:
            print("Performance is consistently stable across test runs. ğŸ“Š")

except Exception as e:
    print(f"âŒ Error generating nightly summary: {e}")
    print("Please check the nightly artifact for detailed results.")
PY
          else
            echo "âŒ **Nightly results not available**" >> $GITHUB_STEP_SUMMARY
            echo "The comprehensive performance test encountered issues." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Notify on significant regression  
        if: failure()
        run: |
          echo "ğŸš¨ NIGHTLY PERFORMANCE REGRESSION DETECTED"
          echo "=========================================="
          echo ""
          echo "The nightly comprehensive performance analysis has detected"
          echo "significant performance regression in StrataRegula core components."
          echo ""
          echo "This indicates a systemic performance issue that requires immediate attention."
          echo ""
          echo "ğŸ“‹ Next Steps:"
          echo "1. Review the comprehensive analysis in the job summary"
          echo "2. Check recent commits for performance-impacting changes" 
          echo "3. Run local benchmarks to reproduce the issue"
          echo "4. Consider reverting recent changes if regression is severe"
          echo ""
          echo "ğŸ“Š Full results are available in the nightly artifacts."