name: Bench Guard
# StrataRegula ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
# ä¸»è¦æ©Ÿèƒ½ï¼ˆPatternExpander, ConfigCompiler, Kernelï¼‰ã®æœ€é©åŒ–å®Ÿè£…ã¨
# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚’æ¯”è¼ƒã—ã€ã—ãã„å€¤ã‚’ä¸‹å›ã£ãŸå ´åˆã«PRã‚’è½ã¨ã™

on:
  pull_request:
    paths:
      - 'strataregula/**'
      - 'scripts/**'
      - 'pyproject.toml'
      - 'requirements.txt'
  push:
    branches: [ main, develop ]
    paths:
      - 'strataregula/**'
      - 'scripts/**'
      - 'pyproject.toml'
      - 'requirements.txt'

jobs:
  bench-guard:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      TZ: Asia/Tokyo
      PYTHONUTF8: "1"
      # StrataRegula ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã§èª¿æ•´å¯èƒ½ï¼‰
      SR_BENCH_MIN_RATIO: "50"      # 50x ã®é€Ÿåº¦æ¯”ã‚’è¦æ±‚ï¼ˆé«˜æ€§èƒ½è¦æ±‚ï¼‰
      SR_BENCH_MAX_P95_US: "50"     # p95 50usä»¥ä¸‹ã‚’è¦æ±‚ï¼ˆä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¦æ±‚ï¼‰
      SR_BENCH_N: "50000"           # 5ä¸‡å›å®Ÿè¡Œã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
      SR_BENCH_WARMUP: "1000"       # 1000å›ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'  # StrataRegula ã®æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # ã‚¨ãƒ‡ã‚£ã‚¿ãƒ–ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
          pip install -e .
          
          # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Verify StrataRegula components
        run: |
          python -c "
          try:
              import strataregula
              print(f'âœ… StrataRegula v{strataregula.__version__} loaded successfully')
              
              # ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç¢ºèª
              from strataregula.core.pattern_expander import PatternExpander, EnhancedPatternExpander
              print('âœ… PatternExpander components available')
              
              from strataregula.core.config_compiler import ConfigCompiler
              print('âœ… ConfigCompiler available')
              
              from strataregula.kernel import Kernel, LRUCacheBackend
              print('âœ… Kernel and cache backend available')
              
              # å‹•ä½œç¢ºèª
              expander = EnhancedPatternExpander()
              print('âœ… Enhanced pattern expander instantiated successfully')
              
          except Exception as e:
              print(f'âŒ StrataRegula component verification failed: {e}')
              import traceback
              traceback.print_exc()
              exit(1)
          "

      - name: Run performance benchmark (3-run median evaluation)
        id: bench
        run: |
          echo "Starting StrataRegula bench guard performance test with 3-run median evaluation..."
          
          # 3å›å®Ÿè¡Œã—ã¦çµæœã‚’åé›†
          for run in 1 2 3; do
            echo "ğŸ”„ Running benchmark iteration $run/3..."
            
            # å‰å›ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            if [ -f bench_guard.json ]; then
              mv bench_guard.json "bench_guard_run${run}.json"
            fi
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            python scripts/bench_guard.py || {
              echo "âš ï¸ Benchmark run $run failed, continuing with remaining runs..."
              echo "{\"overall\":{\"min_ratio\":0,\"max_p95_us\":9999},\"passed\":false,\"error\":\"execution_failed\"}" > "bench_guard_run${run}.json"
            }
            
            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ã„åå‰ã«ãƒªãƒãƒ¼ãƒ 
            if [ -f bench_guard.json ]; then
              mv bench_guard.json "bench_guard_run${run}.json"
            fi
            
            echo "ğŸ“Š Run $run completed"
          done
          
          # 3ã¤ã®çµæœã‹ã‚‰ä¸­å¤®å€¤ã‚’è¨ˆç®—ã—ã¦æœ€çµ‚åˆ¤å®š
          echo "ğŸ§® Calculating median results from 3 runs..."
          python - <<'PY'
          import json
          import glob
          import statistics
          import sys
          import os
          
          try:
              # 3ã¤ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
              run_files = sorted(glob.glob("bench_guard_run*.json"))
              if len(run_files) != 3:
                  print(f"âŒ Expected 3 result files, found {len(run_files)}")
                  sys.exit(1)
              
              runs = []
              for file in run_files:
                  try:
                      with open(file) as f:
                          data = json.load(f)
                          runs.append(data)
                      print(f"âœ… Loaded {file}: ratio={data.get('overall', {}).get('min_ratio', 0):.2f}x, p95={data.get('overall', {}).get('max_p95_us', 9999):.1f}Î¼s")
                  except Exception as e:
                      print(f"âš ï¸ Failed to load {file}: {e}")
                      # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ã‚’ä½¿ç”¨
                      runs.append({"overall":{"min_ratio":0,"max_p95_us":9999},"passed":False,"error":"parse_failed"})
              
              # ä¸­å¤®å€¤è¨ˆç®—
              ratios = [r.get("overall", {}).get("min_ratio", 0) for r in runs]
              p95s = [r.get("overall", {}).get("max_p95_us", 9999) for r in runs]
              
              median_ratio = statistics.median(ratios)
              median_p95 = statistics.median(p95s)
              
              # é–¾å€¤å–å¾—
              min_ratio_threshold = float(os.getenv("SR_BENCH_MIN_RATIO", "50"))
              max_p95_threshold = float(os.getenv("SR_BENCH_MAX_P95_US", "50"))
              
              # æœ€çµ‚åˆ¤å®š
              ratio_ok = median_ratio >= min_ratio_threshold
              p95_ok = median_p95 <= max_p95_threshold
              final_passed = ratio_ok and p95_ok
              
              print(f"\nğŸ“‹ Median Results:")
              print(f"  - Median ratio: {median_ratio:.2f}x (threshold: â‰¥{min_ratio_threshold}x) {'âœ…' if ratio_ok else 'âŒ'}")
              print(f"  - Median p95: {median_p95:.1f}Î¼s (threshold: â‰¤{max_p95_threshold}Î¼s) {'âœ…' if p95_ok else 'âŒ'}")
              print(f"  - Final result: {'âœ… PASSED' if final_passed else 'âŒ FAILED'}")
              
              # æœ€çµ‚çµæœã‚’GitHub Actionså‡ºåŠ›ã«è¨­å®š
              print(f"OVERALL_RATIO={median_ratio:.2f}")
              print(f"OVERALL_P95={median_p95:.1f}")
              print(f"PASSED={str(final_passed).lower()}")
              print(f"RATIO_OK={str(ratio_ok).lower()}")
              print(f"P95_OK={str(p95_ok).lower()}")
              
              # çµ±åˆçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
              # æœ€ã‚‚ä»£è¡¨çš„ãªçµæœï¼ˆä¸­å¤®å€¤ã«æœ€ã‚‚è¿‘ã„ï¼‰ã‚’é¸æŠ
              closest_run = min(runs, key=lambda r: abs(r.get("overall", {}).get("min_ratio", 0) - median_ratio))
              
              # ä¸­å¤®å€¤çµæœã§æ›´æ–°
              final_result = closest_run.copy()
              final_result["median_evaluation"] = {
                  "runs_count": len(runs),
                  "individual_ratios": ratios,
                  "individual_p95s": p95s,
                  "median_ratio": median_ratio,
                  "median_p95": median_p95,
                  "evaluation_method": "3-run median"
              }
              final_result["overall"] = {
                  "min_ratio": median_ratio,
                  "max_p95_us": median_p95,
                  "ratio_ok": ratio_ok,
                  "fast_p95_ok": p95_ok
              }
              final_result["passed"] = final_passed
              
              # çµ±åˆçµæœã‚’ä¿å­˜
              with open("bench_guard.json", "w") as f:
                  json.dump(final_result, f, indent=2, ensure_ascii=False)
              
              print(f"\nğŸ’¾ Final results saved to bench_guard.json")
              
              # å¤±æ•—æ™‚ã¯exit 1
              if not final_passed:
                  print(f"\nğŸš¨ Performance regression detected in median evaluation")
                  if not ratio_ok:
                      print(f"   - Speed ratio {median_ratio:.2f}x < {min_ratio_threshold}x")
                  if not p95_ok:
                      print(f"   - P95 latency {median_p95:.1f}Î¼s > {max_p95_threshold}Î¼s")
                  sys.exit(1)
              else:
                  print(f"\nâœ… All median performance thresholds met")
                  
          except Exception as e:
              print(f"âŒ Error in median evaluation: {e}")
              import traceback
              traceback.print_exc()
              sys.exit(1)
          PY

      - name: Upload benchmark artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: strataregula-bench-guard-results
          path: bench_guard.json
          retention-days: 30

      - name: Create job summary
        if: always()
        run: |
          echo "## ğŸš€ StrataRegula Bench Guard Result (JST)" >> $GITHUB_STEP_SUMMARY
          echo "_Performance regression test completed at $(date '+%Y-%m-%d %H:%M:%S JST')_" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f bench_guard.json ]; then
            python - <<'PY' >> $GITHUB_STEP_SUMMARY
import json
import pathlib

try:
    data = json.loads(pathlib.Path("bench_guard.json").read_text())
    
    # çµæœã‚µãƒãƒªãƒ¼
    passed = data['passed']
    status_emoji = "âœ…" if passed else "âŒ"
    status_text = "PASSED" if passed else "FAILED"
    
    print(f"### {status_emoji} Result: **{status_text}**")
    print()
    
    # å€‹åˆ¥ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
    benchmarks = data['benchmarks']
    overall = data['overall']
    config = data['config']
    
    print("### ğŸ“Š Component Performance")
    print("| Component | Fast (ops/s) | Slow (ops/s) | Ratio | p95 (Î¼s) |")
    print("|-----------|--------------|--------------|-------|----------|")
    
    for name, bench in benchmarks.items():
        fast_ops = bench['fast']['ops']
        slow_ops = bench['slow']['ops']
        ratio = bench['ratio']
        p95 = bench['fast']['p95_us']
        display_name = name.replace('_', ' ').title()
        print(f"| **{display_name}** | {fast_ops:,.0f} | {slow_ops:,.0f} | **{ratio:.1f}x** | {p95:.1f}Î¼s |")
    
    print()
    
    # ç·åˆè©•ä¾¡
    print("### ğŸ¯ Overall Assessment")
    ratio_ok = overall['ratio_ok']
    p95_ok = overall['fast_p95_ok']
    
    ratio_emoji = "âœ…" if ratio_ok else "âŒ"
    p95_emoji = "âœ…" if p95_ok else "âŒ"
    
    print(f"- {ratio_emoji} **Minimum Speed Ratio**: {overall['min_ratio']:.1f}x (required: â‰¥{config['min_ratio']}x)")
    print(f"- {p95_emoji} **Maximum p95 Latency**: {overall['max_p95_us']:.1f}Î¼s (required: â‰¤{config['max_p95_us']}Î¼s)")
    print()
    
    # è¨­å®šæƒ…å ±
    print("### âš™ï¸ Test Configuration")
    print(f"- **Test iterations**: {config['n']:,}")
    print(f"- **Warmup iterations**: {config['warmup']:,}")
    print(f"- **Components tested**: PatternExpander, ConfigCompiler, KernelCache")
    print(f"- **Timestamp**: {data['timestamp']}")
    
    if not passed:
        print()
        print("### âŒ Performance Regression Detected")
        print("**This PR introduces performance regressions and will be blocked.**")
        print()
        
        reasons = []
        if not ratio_ok:
            print(f"- **Speed Degradation**: Minimum ratio {overall['min_ratio']:.1f}x is below the required threshold of {config['min_ratio']}x")
        if not p95_ok:
            print(f"- **Latency Regression**: Maximum p95 latency {overall['max_p95_us']:.1f}Î¼s exceeds the threshold of {config['max_p95_us']}Î¼s")
        
        print()
        print("**ğŸ’¡ Next Steps:**")
        print("1. Profile the affected components to identify performance bottlenecks")
        print("2. Optimize the slow paths or review algorithmic changes")
        print("3. Consider if the performance regression is acceptable and adjust thresholds")
        print()
        print("**ğŸ”§ Emergency Threshold Adjustment (use sparingly):**")
        print("```yaml")
        print("env:")
        print(f"  SR_BENCH_MIN_RATIO: \"30\"  # Temporarily lower from {config['min_ratio']}")
        print(f"  SR_BENCH_MAX_P95_US: \"100\" # Temporarily higher from {config['max_p95_us']}")
        print("```")
    else:
        print()
        print("### âœ… Performance Maintained")
        print("All components meet the required performance thresholds. Great work! ğŸ‰")

except Exception as e:
    print(f"âŒ Error processing benchmark results: {e}")
    print()
    print("Please check the benchmark artifact for detailed information.")
PY
          else
            echo "âŒ **Benchmark results file not found**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The performance test may have failed to run properly. Check the job logs for details." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Performance regression detected
        if: failure()
        run: |
          echo "ğŸš¨ StrataRegula performance regression detected!"
          echo ""
          echo "One or more core components failed to meet performance requirements:"
          echo "- PatternExpander optimization degraded"
          echo "- ConfigCompiler performance regressed"  
          echo "- Kernel cache efficiency decreased"
          echo ""
          echo "This PR will be blocked until performance is restored."
          echo ""
          echo "To temporarily adjust thresholds (use only in emergencies):"
          echo "  - SR_BENCH_MIN_RATIO: Minimum speed ratio (default: 50)"
          echo "  - SR_BENCH_MAX_P95_US: Maximum p95 latency in Î¼s (default: 50)"
          echo ""
          echo "Consider:"
          echo "1. Profiling affected components"
          echo "2. Reviewing algorithmic changes"
          echo "3. Optimizing critical paths"
          exit 1