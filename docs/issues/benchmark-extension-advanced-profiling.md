# [0.5] ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ‹¡å¼µ - é«˜åº¦ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµ±åˆ

**Labels**: `backlog`, `target-0.5`, `performance`, `benchmarking`, `enhancement`, `priority-p2`  
**Milestone**: `v0.5.0`  
**Priority**: P2 (ä¾¡å€¤å‘ä¸Šå®Ÿè£…)

## ğŸ“‹ ç›®çš„
ç¾åœ¨ã®CPUæ™‚é–“ä¸­å¿ƒã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ‹¡å¼µã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã€ä¸¦è¡Œå‡¦ç†æ€§èƒ½æ¸¬å®šã€ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆçµ±åˆã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ä½“ç³»ã‚’æ§‹ç¯‰ã€‚ã‚·ã‚¹ãƒ†ãƒ ã®é™ç•Œã¨æœ€é©åŒ–æ©Ÿä¼šã‚’å¤šè§’çš„ã«åˆ†æå¯èƒ½ã«ã™ã‚‹ã€‚

## ğŸ¯ å…·ä½“çš„ãªä»•æ§˜

### æ‹¡å¼µãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä½“ç³»
```
Extended Benchmark Framework
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Benchmark Categories                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚                                       â”‚
â”‚  1. CPU Performance  â”‚  ç¾åœ¨ã®å®Ÿè£… (perf_suite)            â”‚
â”‚     - Execution Time â”‚  - P50/P95 percentiles               â”‚
â”‚     - Throughput     â”‚  - Speedup ratios                    â”‚
â”‚                      â”‚                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚                                       â”‚
â”‚  2. Memory Profile   â”‚  NEW: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ†æ              â”‚
â”‚     - Heap Usage     â”‚  - Peak memory consumption           â”‚
â”‚     - Allocations    â”‚  - Allocation patterns               â”‚
â”‚     - GC Pressure    â”‚  - Garbage collection impact         â”‚
â”‚                      â”‚                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚                                       â”‚
â”‚  3. Concurrency      â”‚  NEW: ä¸¦è¡Œå‡¦ç†æ€§èƒ½                  â”‚
â”‚     - Thread Safety  â”‚  - Race condition detection          â”‚
â”‚     - Scalability    â”‚  - Multi-core utilization            â”‚
â”‚     - Lock Contentionâ”‚  - Synchronization overhead          â”‚
â”‚                      â”‚                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚                                       â”‚
â”‚  4. Stress Testing   â”‚  NEW: é™ç•Œæ€§èƒ½æ¸¬å®š                  â”‚
â”‚     - Load Testing   â”‚  - Maximum throughput                â”‚
â”‚     - Endurance      â”‚  - Long-running stability            â”‚
â”‚     - Spike Testing  â”‚  - Burst load handling               â”‚
â”‚                      â”‚                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ æŠ€è¡“å®Ÿè£…

### 1. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
```python
# performance/benchmarks/memory_profiling.py
import tracemalloc
import gc
from memory_profiler import profile, memory_usage
from typing import Dict, Any, Callable
import psutil
import numpy as np

class MemoryBenchmark:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.initial_memory = None
        self.snapshots = []
        
    def profile_memory_usage(self, benchmark_func: Callable, 
                            iterations: int = 100) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
        
        # Garbage Collectionç„¡åŠ¹åŒ–ï¼ˆæ­£ç¢ºãªæ¸¬å®šã®ãŸã‚ï¼‰
        gc_was_enabled = gc.isenabled()
        gc.disable()
        
        try:
            # tracemallocé–‹å§‹
            tracemalloc.start()
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒª
            self.initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¿½è·¡
            memory_samples = []
            peak_memory = 0
            
            for i in range(iterations):
                # å®Ÿè¡Œå‰ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                snapshot_before = tracemalloc.take_snapshot()
                
                # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
                result = benchmark_func()
                
                # å®Ÿè¡Œå¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                snapshot_after = tracemalloc.take_snapshot()
                
                # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
                current_memory = self.process.memory_info().rss / 1024 / 1024
                memory_samples.append(current_memory - self.initial_memory)
                peak_memory = max(peak_memory, current_memory)
                
                # å·®åˆ†çµ±è¨ˆ
                top_stats = snapshot_after.compare_to(
                    snapshot_before, 'lineno'
                )
                
                # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºç”¨ã«ä¿å­˜
                if i % 10 == 0:
                    self.snapshots.append({
                        'iteration': i,
                        'snapshot': snapshot_after,
                        'memory_mb': current_memory
                    })
            
            # GCçµ±è¨ˆ
            gc.collect()
            gc_stats = gc.get_stats()
            
            # çµæœé›†è¨ˆ
            return {
                'memory_stats': {
                    'initial_mb': self.initial_memory,
                    'peak_mb': peak_memory,
                    'average_mb': np.mean(memory_samples),
                    'std_mb': np.std(memory_samples),
                    'p95_mb': np.percentile(memory_samples, 95)
                },
                'allocation_stats': self._analyze_allocations(top_stats[:10]),
                'gc_stats': {
                    'collections': gc_stats[-1]['collections'] if gc_stats else 0,
                    'collected': gc_stats[-1]['collected'] if gc_stats else 0,
                    'uncollectable': gc_stats[-1]['uncollectable'] if gc_stats else 0
                },
                'memory_growth': self._detect_memory_leak(memory_samples)
            }
            
        finally:
            tracemalloc.stop()
            if gc_was_enabled:
                gc.enable()
    
    def _analyze_allocations(self, top_stats) -> list:
        """ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ"""
        allocations = []
        for stat in top_stats:
            allocations.append({
                'file': stat.traceback[0].filename,
                'line': stat.traceback[0].lineno,
                'size_diff_mb': stat.size_diff / 1024 / 1024,
                'count_diff': stat.count_diff
            })
        return allocations
    
    def _detect_memory_leak(self, memory_samples: list) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º"""
        if len(memory_samples) < 10:
            return {'leak_detected': False}
        
        # ç·šå½¢å›å¸°ã§ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        x = np.arange(len(memory_samples))
        coefficients = np.polyfit(x, memory_samples, 1)
        slope = coefficients[0]
        
        # æ­£ã®å‚¾ããŒç¶™ç¶šçš„ãªå ´åˆãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§
        leak_detected = slope > 0.01  # 1iterationå½“ãŸã‚Š0.01MBä»¥ä¸Šã®å¢—åŠ 
        
        return {
            'leak_detected': leak_detected,
            'growth_rate_mb_per_iteration': slope,
            'projected_100k_iterations_mb': slope * 100000
        }

    @profile
    def profile_line_by_line(self, benchmark_func):
        """è¡Œå˜ä½ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ä½¿ç”¨ï¼‰"""
        return benchmark_func()
```

### 2. ä¸¦è¡Œå‡¦ç†æ€§èƒ½æ¸¬å®š
```python
# performance/benchmarks/concurrency_profiling.py
import concurrent.futures
import threading
import multiprocessing
import asyncio
import time
from typing import Dict, Any, Callable, List
import numpy as np

class ConcurrencyBenchmark:
    """ä¸¦è¡Œå‡¦ç†æ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    
    def __init__(self):
        self.lock_contention_stats = {}
        self.thread_execution_times = {}
        
    def benchmark_threading(self, benchmark_func: Callable, 
                          thread_counts: List[int] = [1, 2, 4, 8, 16]) -> Dict[str, Any]:
        """ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰æ€§èƒ½æ¸¬å®š"""
        results = {}
        
        for num_threads in thread_counts:
            start_time = time.perf_counter()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                # ä¸¦è¡Œå®Ÿè¡Œ
                futures = [
                    executor.submit(benchmark_func) 
                    for _ in range(num_threads * 10)
                ]
                
                # çµæœåé›†
                thread_results = [f.result() for f in futures]
            
            elapsed_time = time.perf_counter() - start_time
            
            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ
            results[f'threads_{num_threads}'] = {
                'total_time_seconds': elapsed_time,
                'throughput_per_second': len(futures) / elapsed_time,
                'average_time_per_task': elapsed_time / len(futures),
                'speedup': results.get('threads_1', {}).get('total_time_seconds', elapsed_time) / elapsed_time if num_threads > 1 else 1.0,
                'efficiency': (results.get('threads_1', {}).get('total_time_seconds', elapsed_time) / elapsed_time / num_threads) if num_threads > 1 else 1.0
            }
        
        return {
            'threading_results': results,
            'scalability_analysis': self._analyze_scalability(results),
            'optimal_thread_count': self._find_optimal_threads(results)
        }
    
    def benchmark_multiprocessing(self, benchmark_func: Callable,
                                process_counts: List[int] = [1, 2, 4, 8]) -> Dict[str, Any]:
        """ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æ€§èƒ½æ¸¬å®š"""
        results = {}
        
        for num_processes in process_counts:
            start_time = time.perf_counter()
            
            with multiprocessing.Pool(processes=num_processes) as pool:
                # ä¸¦è¡Œå®Ÿè¡Œ
                tasks = range(num_processes * 10)
                process_results = pool.map(benchmark_func, tasks)
            
            elapsed_time = time.perf_counter() - start_time
            
            results[f'processes_{num_processes}'] = {
                'total_time_seconds': elapsed_time,
                'throughput_per_second': len(tasks) / elapsed_time,
                'speedup': results.get('processes_1', {}).get('total_time_seconds', elapsed_time) / elapsed_time if num_processes > 1 else 1.0
            }
        
        return {
            'multiprocessing_results': results,
            'cpu_utilization': self._measure_cpu_utilization(results),
            'ipc_overhead': self._estimate_ipc_overhead(results)
        }
    
    async def benchmark_async(self, async_benchmark_func: Callable,
                            concurrency_levels: List[int] = [10, 50, 100, 500]) -> Dict[str, Any]:
        """éåŒæœŸå‡¦ç†æ€§èƒ½æ¸¬å®š"""
        results = {}
        
        for concurrency in concurrency_levels:
            start_time = time.perf_counter()
            
            # éåŒæœŸã‚¿ã‚¹ã‚¯ä½œæˆ
            tasks = [async_benchmark_func() for _ in range(concurrency)]
            
            # ä¸¦è¡Œå®Ÿè¡Œ
            async_results = await asyncio.gather(*tasks)
            
            elapsed_time = time.perf_counter() - start_time
            
            results[f'async_{concurrency}'] = {
                'total_time_seconds': elapsed_time,
                'throughput_per_second': concurrency / elapsed_time,
                'average_latency_ms': (elapsed_time / concurrency) * 1000
            }
        
        return {
            'async_results': results,
            'concurrency_analysis': self._analyze_async_scalability(results)
        }
    
    def detect_race_conditions(self, benchmark_func: Callable, 
                              iterations: int = 1000) -> Dict[str, Any]:
        """ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æ¤œå‡º"""
        shared_state = {'counter': 0, 'errors': []}
        lock = threading.Lock()
        
        def wrapped_func():
            try:
                # æ„å›³çš„ã«ãƒ­ãƒƒã‚¯ãªã—ã§ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆæ¤œå‡ºç”¨ï¼‰
                temp = shared_state['counter']
                time.sleep(0.0001)  # ãƒ¬ãƒ¼ã‚¹ç™ºç”Ÿç¢ºç‡ä¸Šæ˜‡
                shared_state['counter'] = temp + 1
                
                # æœ¬æ¥ã®å‡¦ç†
                return benchmark_func()
            except Exception as e:
                shared_state['errors'].append(str(e))
        
        # ä¸¦è¡Œå®Ÿè¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(wrapped_func) for _ in range(iterations)]
            results = [f.result() for f in futures]
        
        # ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æ¤œå‡º
        expected_counter = iterations
        actual_counter = shared_state['counter']
        race_detected = actual_counter != expected_counter
        
        return {
            'race_condition_detected': race_detected,
            'expected_counter': expected_counter,
            'actual_counter': actual_counter,
            'lost_updates': expected_counter - actual_counter,
            'thread_safety_score': actual_counter / expected_counter,
            'errors': shared_state['errors']
        }
    
    def _analyze_scalability(self, results: Dict) -> Dict[str, Any]:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ"""
        thread_counts = []
        speedups = []
        
        for key, value in results.items():
            if 'threads_' in key:
                count = int(key.split('_')[1])
                thread_counts.append(count)
                speedups.append(value.get('speedup', 1.0))
        
        # ã‚¢ãƒ ãƒ€ãƒ¼ãƒ«ã®æ³•å‰‡ã«ã‚ˆã‚‹ç†è«–å€¤è¨ˆç®—
        if len(speedups) > 1:
            # ä¸¦åˆ—åŒ–å¯èƒ½éƒ¨åˆ†ã®æ¨å®š
            p = 0.9  # ä»®å®š: 90%ãŒä¸¦åˆ—åŒ–å¯èƒ½
            theoretical_speedups = [1 / ((1 - p) + p / n) for n in thread_counts]
            
            return {
                'actual_speedups': dict(zip(thread_counts, speedups)),
                'theoretical_speedups': dict(zip(thread_counts, theoretical_speedups)),
                'scalability_efficiency': np.mean([a/t for a, t in zip(speedups, theoretical_speedups)]),
                'parallel_fraction_estimate': p
            }
        
        return {}
```

### 3. ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆçµ±åˆ
```python
# performance/benchmarks/stress_testing.py
import time
import random
from typing import Dict, Any, Callable, Optional
import numpy as np
from dataclasses import dataclass
import psutil

@dataclass
class StressTestConfig:
    """ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆè¨­å®š"""
    duration_seconds: int = 60
    initial_load: int = 10
    max_load: int = 1000
    ramp_up_seconds: int = 10
    spike_multiplier: float = 5.0
    endurance_hours: float = 1.0

class StressTesting:
    """ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self):
        self.results = []
        self.errors = []
        self.system_metrics = []
        
    def load_test(self, benchmark_func: Callable, 
                 config: StressTestConfig) -> Dict[str, Any]:
        """è² è·ãƒ†ã‚¹ãƒˆ - æ®µéšçš„ã«è² è·ã‚’å¢—åŠ """
        
        start_time = time.time()
        current_load = config.initial_load
        load_step = (config.max_load - config.initial_load) / config.ramp_up_seconds
        
        results_timeline = []
        
        while time.time() - start_time < config.duration_seconds:
            iteration_start = time.time()
            
            # ç¾åœ¨ã®è² è·ãƒ¬ãƒ™ãƒ«ã§å®Ÿè¡Œ
            concurrent_results = []
            errors_in_iteration = 0
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=int(current_load)) as executor:
                futures = [executor.submit(benchmark_func) for _ in range(int(current_load))]
                
                for future in concurrent.futures.as_completed(futures):
                    try:
                        result = future.result(timeout=1.0)
                        concurrent_results.append(result)
                    except Exception as e:
                        errors_in_iteration += 1
                        self.errors.append({
                            'timestamp': time.time(),
                            'load_level': current_load,
                            'error': str(e)
                        })
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            iteration_time = time.time() - iteration_start
            throughput = len(concurrent_results) / iteration_time if iteration_time > 0 else 0
            
            results_timeline.append({
                'timestamp': time.time() - start_time,
                'load_level': current_load,
                'throughput': throughput,
                'response_time_avg': iteration_time / len(concurrent_results) if concurrent_results else 0,
                'success_rate': len(concurrent_results) / current_load if current_load > 0 else 0,
                'errors': errors_in_iteration,
                'cpu_percent': psutil.cpu_percent(interval=0.1),
                'memory_percent': psutil.virtual_memory().percent
            })
            
            # è² è·å¢—åŠ ï¼ˆãƒ©ãƒ³ãƒ—ã‚¢ãƒƒãƒ—æœŸé–“ä¸­ï¼‰
            if time.time() - start_time < config.ramp_up_seconds:
                current_load = min(current_load + load_step, config.max_load)
            
            time.sleep(0.1)  # åˆ¶å¾¡é–“éš”
        
        return {
            'test_type': 'load_test',
            'duration': config.duration_seconds,
            'max_load_achieved': max([r['load_level'] for r in results_timeline]),
            'max_throughput': max([r['throughput'] for r in results_timeline]),
            'average_success_rate': np.mean([r['success_rate'] for r in results_timeline]),
            'error_rate': len(self.errors) / sum([r['load_level'] for r in results_timeline]),
            'breaking_point': self._find_breaking_point(results_timeline),
            'timeline': results_timeline
        }
    
    def spike_test(self, benchmark_func: Callable,
                  config: StressTestConfig) -> Dict[str, Any]:
        """ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ†ã‚¹ãƒˆ - çªç™ºçš„ãªè² è·å¢—åŠ ã¸ã®å¯¾å¿œæ¸¬å®š"""
        
        normal_load = config.initial_load
        spike_load = int(normal_load * config.spike_multiplier)
        
        results = {
            'normal_performance': [],
            'spike_performance': [],
            'recovery_performance': []
        }
        
        # Phase 1: é€šå¸¸è² è·
        print("Phase 1: Normal load...")
        for _ in range(10):
            result = self._execute_load_iteration(benchmark_func, normal_load)
            results['normal_performance'].append(result)
            time.sleep(1)
        
        # Phase 2: ã‚¹ãƒ‘ã‚¤ã‚¯è² è·
        print("Phase 2: Spike load...")
        spike_start = time.time()
        for _ in range(5):
            result = self._execute_load_iteration(benchmark_func, spike_load)
            results['spike_performance'].append(result)
            time.sleep(1)
        spike_duration = time.time() - spike_start
        
        # Phase 3: å›å¾©æœŸ
        print("Phase 3: Recovery...")
        recovery_start = time.time()
        for _ in range(10):
            result = self._execute_load_iteration(benchmark_func, normal_load)
            results['recovery_performance'].append(result)
            time.sleep(1)
        recovery_duration = time.time() - recovery_start
        
        # åˆ†æ
        normal_throughput = np.mean([r['throughput'] for r in results['normal_performance']])
        spike_throughput = np.mean([r['throughput'] for r in results['spike_performance']])
        recovery_throughput = np.mean([r['throughput'] for r in results['recovery_performance']])
        
        return {
            'test_type': 'spike_test',
            'spike_multiplier': config.spike_multiplier,
            'normal_throughput': normal_throughput,
            'spike_throughput': spike_throughput,
            'throughput_degradation': (normal_throughput - spike_throughput) / normal_throughput,
            'recovery_throughput': recovery_throughput,
            'recovery_time_seconds': recovery_duration,
            'recovery_efficiency': recovery_throughput / normal_throughput,
            'spike_handling_score': spike_throughput / (normal_throughput * config.spike_multiplier),
            'detailed_results': results
        }
    
    def endurance_test(self, benchmark_func: Callable,
                      config: StressTestConfig) -> Dict[str, Any]:
        """è€ä¹…ãƒ†ã‚¹ãƒˆ - é•·æ™‚é–“å®Ÿè¡Œã§ã®å®‰å®šæ€§æ¸¬å®š"""
        
        duration_seconds = config.endurance_hours * 3600
        checkpoint_interval = 300  # 5åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        
        checkpoints = []
        start_time = time.time()
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        while time.time() - start_time < duration_seconds:
            checkpoint_start = time.time()
            
            # å®šå¸¸è² è·å®Ÿè¡Œ
            results = []
            for _ in range(100):
                try:
                    result = benchmark_func()
                    results.append(result)
                except Exception as e:
                    self.errors.append({
                        'timestamp': time.time() - start_time,
                        'error': str(e)
                    })
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨˜éŒ²
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024
            checkpoint = {
                'timestamp_hours': (time.time() - start_time) / 3600,
                'throughput': len(results) / (time.time() - checkpoint_start),
                'memory_mb': current_memory,
                'memory_growth_mb': current_memory - initial_memory,
                'error_count': len(self.errors),
                'cpu_percent': psutil.cpu_percent(interval=1),
                'thread_count': threading.active_count()
            }
            checkpoints.append(checkpoint)
            
            # æ¬¡ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¾ã§å¾…æ©Ÿ
            time.sleep(max(0, checkpoint_interval - (time.time() - checkpoint_start)))
        
        # å®‰å®šæ€§åˆ†æ
        throughputs = [c['throughput'] for c in checkpoints]
        memory_values = [c['memory_mb'] for c in checkpoints]
        
        return {
            'test_type': 'endurance_test',
            'duration_hours': config.endurance_hours,
            'checkpoints': checkpoints,
            'stability_analysis': {
                'throughput_stability': 1 - (np.std(throughputs) / np.mean(throughputs)),
                'memory_leak_detected': self._detect_memory_trend(memory_values),
                'total_errors': len(self.errors),
                'error_rate_per_hour': len(self.errors) / config.endurance_hours,
                'performance_degradation': (throughputs[0] - throughputs[-1]) / throughputs[0] if throughputs else 0
            }
        }
    
    def _execute_load_iteration(self, benchmark_func: Callable, 
                               load_level: int) -> Dict[str, Any]:
        """è² è·ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        start_time = time.time()
        success_count = 0
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=load_level) as executor:
            futures = [executor.submit(benchmark_func) for _ in range(load_level)]
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result(timeout=1.0)
                    success_count += 1
                except:
                    pass
        
        elapsed = time.time() - start_time
        
        return {
            'load_level': load_level,
            'throughput': success_count / elapsed if elapsed > 0 else 0,
            'success_rate': success_count / load_level if load_level > 0 else 0
        }
    
    def _find_breaking_point(self, timeline: List[Dict]) -> Optional[Dict[str, Any]]:
        """ã‚·ã‚¹ãƒ†ãƒ ã®é™ç•Œç‚¹æ¤œå‡º"""
        for i in range(1, len(timeline)):
            if timeline[i]['success_rate'] < 0.95:  # 95%æˆåŠŸç‡ã‚’ä¸‹å›ã‚‹ç‚¹
                return {
                    'load_level': timeline[i]['load_level'],
                    'timestamp': timeline[i]['timestamp'],
                    'success_rate': timeline[i]['success_rate']
                }
        return None
    
    def _detect_memory_trend(self, memory_values: List[float]) -> bool:
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º"""
        if len(memory_values) < 3:
            return False
        
        # ç·šå½¢å›å¸°ã§å¢—åŠ å‚¾å‘ã‚’æ¤œå‡º
        x = np.arange(len(memory_values))
        slope = np.polyfit(x, memory_values, 1)[0]
        
        # 1æ™‚é–“å½“ãŸã‚Š100MBä»¥ä¸Šã®å¢—åŠ ã§ãƒªãƒ¼ã‚¯ã¨åˆ¤å®š
        return slope > (100 / len(memory_values))
```

### çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ
```python
# performance/benchmarks/extended_suite.py
class ExtendedBenchmarkSuite:
    """æ‹¡å¼µãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆçµ±åˆ"""
    
    def __init__(self):
        self.memory_benchmark = MemoryBenchmark()
        self.concurrency_benchmark = ConcurrencyBenchmark()
        self.stress_testing = StressTesting()
        
    def run_comprehensive_benchmark(self, benchmark_func: Callable) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        
        print("ğŸš€ Starting Comprehensive Benchmark Suite...")
        
        results = {
            'timestamp': datetime.utcnow().isoformat(),
            'benchmark_function': benchmark_func.__name__
        }
        
        # 1. CPU Performance (æ—¢å­˜)
        print("ğŸ“Š Phase 1/4: CPU Performance...")
        cpu_results = self.run_cpu_benchmark(benchmark_func)
        results['cpu_performance'] = cpu_results
        
        # 2. Memory Profiling
        print("ğŸ’¾ Phase 2/4: Memory Profiling...")
        memory_results = self.memory_benchmark.profile_memory_usage(benchmark_func)
        results['memory_profile'] = memory_results
        
        # 3. Concurrency Testing
        print("ğŸ”„ Phase 3/4: Concurrency Testing...")
        concurrency_results = self.concurrency_benchmark.benchmark_threading(benchmark_func)
        results['concurrency'] = concurrency_results
        
        # 4. Stress Testing
        print("ğŸ”¥ Phase 4/4: Stress Testing...")
        stress_config = StressTestConfig(
            duration_seconds=60,
            max_load=100
        )
        stress_results = self.stress_testing.load_test(benchmark_func, stress_config)
        results['stress_test'] = stress_results
        
        # ç·åˆã‚¹ã‚³ã‚¢ç®—å‡º
        results['overall_score'] = self.calculate_overall_score(results)
        
        print("âœ… Comprehensive Benchmark Complete!")
        return results
    
    def calculate_overall_score(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ç®—å‡º"""
        
        scores = {
            'cpu_score': 100,  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            'memory_score': 100,
            'concurrency_score': 100,
            'stress_score': 100
        }
        
        # CPU ã‚¹ã‚³ã‚¢ (speedupåŸºæº–)
        if 'cpu_performance' in results:
            speedup = results['cpu_performance'].get('speedup_p50', 15.0)
            scores['cpu_score'] = min(100, (speedup / 15.0) * 100)
        
        # ãƒ¡ãƒ¢ãƒªã‚¹ã‚³ã‚¢ (åŠ¹ç‡åŸºæº–)
        if 'memory_profile' in results:
            peak_mb = results['memory_profile']['memory_stats']['peak_mb']
            leak_detected = results['memory_profile']['memory_growth']['leak_detected']
            scores['memory_score'] = 100 if not leak_detected else 50
            scores['memory_score'] *= min(1.0, 100 / peak_mb)  # 100MBåŸºæº–
        
        # ä¸¦è¡Œå‡¦ç†ã‚¹ã‚³ã‚¢ (ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åŸºæº–)
        if 'concurrency' in results:
            efficiency = results['concurrency']['scalability_analysis'].get('scalability_efficiency', 0.5)
            scores['concurrency_score'] = efficiency * 100
        
        # ã‚¹ãƒˆãƒ¬ã‚¹ã‚¹ã‚³ã‚¢ (æˆåŠŸç‡åŸºæº–)
        if 'stress_test' in results:
            success_rate = results['stress_test']['average_success_rate']
            scores['stress_score'] = success_rate * 100
        
        # ç·åˆã‚¹ã‚³ã‚¢ (åŠ é‡å¹³å‡)
        weights = {
            'cpu_score': 0.3,
            'memory_score': 0.2,
            'concurrency_score': 0.25,
            'stress_score': 0.25
        }
        
        overall = sum(scores[k] * weights[k] for k in scores)
        
        return {
            'individual_scores': scores,
            'weights': weights,
            'overall_score': overall,
            'grade': self._score_to_grade(overall)
        }
    
    def _score_to_grade(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‚’ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¤‰æ›"""
        if score >= 90: return 'A+'
        if score >= 85: return 'A'
        if score >= 80: return 'B+'
        if score >= 75: return 'B'
        if score >= 70: return 'C+'
        if score >= 65: return 'C'
        if score >= 60: return 'D'
        return 'F'
```

## ğŸ“Š æˆåŠŸæŒ‡æ¨™

### æ¸¬å®šç²¾åº¦è¦ä»¶
- **ãƒ¡ãƒ¢ãƒªæ¸¬å®šç²¾åº¦**: Â±1MBä»¥å†…ã®èª¤å·®
- **ä¸¦è¡Œå‡¦ç†æ¸¬å®š**: ãƒã‚¤ã‚¯ãƒ­ç§’ç²¾åº¦ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š
- **ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ**: 1000+ QPSå‡¦ç†èƒ½åŠ›
- **ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: < 5%

### ã‚«ãƒãƒ¬ãƒƒã‚¸è¦ä»¶
- **ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**: Heap/Stack/GCå®Œå…¨ã‚«ãƒãƒ¼
- **ä¸¦è¡Œå‡¦ç†**: Thread/Process/Asyncå…¨æ–¹å¼å¯¾å¿œ
- **ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ**: Load/Spike/Enduranceå…¨ç¨®é¡å®Ÿè£…
- **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **: Linux/Windows/macOSå¯¾å¿œ

### å“è³ªè¦ä»¶
- **ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ**: åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ
- **å¯è¦–åŒ–**: ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆä»˜ããƒ¬ãƒãƒ¼ãƒˆ
- **CIçµ±åˆ**: å…¨ãƒ†ã‚¹ãƒˆç¨®åˆ¥ã®CIçµ±åˆå¯¾å¿œ
- **é–¾å€¤ç®¡ç†**: ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªåˆæ ¼åŸºæº–

## ğŸš« éç›®æ¨™ãƒ»åˆ¶ç´„äº‹é …

### ç¾åœ¨ã®ã‚¹ã‚³ãƒ¼ãƒ—å¤–
- **GPUæ€§èƒ½æ¸¬å®š**: CPU/ãƒ¡ãƒ¢ãƒªã®ã¿ã€GPUæœªå¯¾å¿œ
- **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ€§èƒ½**: ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†ã®ã¿
- **åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ **: å˜ä¸€ãƒãƒ¼ãƒ‰ã®ã¿
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§**: ã‚½ãƒ•ãƒˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã¿
- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç‰¹åŒ–**: æ±ç”¨æ¸¬å®šã®ã¿

### åˆ¶ç´„äº‹é …
- **Pythonåˆ¶é™**: GILã«ã‚ˆã‚‹çœŸã®ä¸¦åˆ—å‡¦ç†åˆ¶é™
- **ãƒ¡ãƒ¢ãƒªç²¾åº¦**: GCã«ã‚ˆã‚‹æ¸¬å®šèª¤å·®
- **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å·®**: OSå›ºæœ‰ã®æ€§èƒ½ç‰¹æ€§
- **ãƒ„ãƒ¼ãƒ«ä¾å­˜**: psutil, memory_profilerå¿…é ˆ

## ğŸ”— é–¢é€£ãƒ»ä¾å­˜ Issues

### å‰ææ¡ä»¶
- âœ… Performance Tools MVP (0.3.0) - åŸºæœ¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åŸºç›¤
- âœ… perf_analyze (0.4.0äºˆå®š) - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°åŸºç›¤
- â³ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦– (0.5.0) - ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆ

### é€£æºæ¨å¥¨
- **Performance Dashboard** - æ‹¡å¼µãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯è¦–åŒ–
- **CIå®Œå…¨çµ±åˆ** - æ‹¡å¼µãƒ†ã‚¹ãƒˆã®CIå®Ÿè¡Œ
- **ãƒãƒ«ãƒãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ** - å„ç’°å¢ƒã§ã®æ€§èƒ½æ¸¬å®š

### å¾Œç¶šå±•é–‹
- **AIãƒ™ãƒ¼ã‚¹æœ€é©åŒ–** (0.6.0) - æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©åŒ–ææ¡ˆ
- **åˆ†æ•£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯** (0.6.0) - ã‚¯ãƒ©ã‚¹ã‚¿ç’°å¢ƒæ¸¬å®š
- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç‰¹åŒ–** (0.7.0) - GPU/TPUå¯¾å¿œ

## ğŸ”„ å®Ÿè£…æˆ¦ç•¥

### Phase 1: ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° (Week 1-2)
1. MemoryBenchmarkã‚¯ãƒ©ã‚¹å®Ÿè£…
2. tracemalloc/memory_profilerçµ±åˆ
3. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
4. åŸºæœ¬çš„ãªãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### Phase 2: ä¸¦è¡Œå‡¦ç†æ¸¬å®š (Week 2-3)
1. ConcurrencyBenchmarkã‚¯ãƒ©ã‚¹å®Ÿè£…
2. Threading/Multiprocessing/Asyncæ¸¬å®š
3. ãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³æ¤œå‡º
4. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ

### Phase 3: ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ (Week 3-4)
1. StressTestingã‚¯ãƒ©ã‚¹å®Ÿè£…
2. Load/Spike/Enduranceãƒ†ã‚¹ãƒˆ
3. é™ç•Œç‚¹æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
4. å®‰å®šæ€§åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

### Phase 4: çµ±åˆãƒ»æœ€é©åŒ– (Week 4-5)
1. ExtendedBenchmarkSuiteçµ±åˆ
2. ç·åˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
3. CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ
4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ä½¿ç”¨ä¾‹æ•´å‚™

## âœ… å®Œäº†æ¡ä»¶ (Definition of Done)

### æŠ€è¡“è¦ä»¶
- [ ] 3ç¨®é¡ã®æ‹¡å¼µãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè£…å®Œäº†
- [ ] çµ±åˆã‚¹ã‚¤ãƒ¼ãƒˆå‹•ä½œç¢ºèª
- [ ] ç·åˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…
- [ ] CIçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†

### æ¸¬å®šè¦ä»¶
- [ ] ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç²¾åº¦æ¤œè¨¼
- [ ] ä¸¦è¡Œå‡¦ç†ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ç¢ºèª
- [ ] ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆé™ç•Œå€¤æ¸¬å®š
- [ ] ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰5%ä»¥å†…ç¢ºèª

### å“è³ªè¦ä»¶
- [ ] å˜ä½“ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ > 85%
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆå…¨ãƒ‘ã‚¹
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†
- [ ] ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å‹•ä½œç¢ºèª

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ä»¶
- [ ] APIä»•æ§˜æ›¸å®Œæˆ
- [ ] ä½¿ç”¨ä¾‹ãƒ»ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ä½œæˆ
- [ ] CIçµ±åˆã‚¬ã‚¤ãƒ‰ä½œæˆ
- [ ] ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ•´å‚™

---

**æ¨å®šå·¥æ•°**: 4-5 weeks  
**æ‹…å½“è€…**: Performance Engineering Team  
**ãƒ¬ãƒ“ãƒ¥ãƒ¯ãƒ¼**: SRE Team + Development Team  
**ä½œæˆæ—¥**: 2025-09-01  
**æœ€çµ‚æ›´æ–°**: 2025-09-01